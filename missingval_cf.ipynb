{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Missing Data Prediction for Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Submitted in partial fulfillment of the requirements for the degree of Master of Analytics, RMIT University, Melbourne*\n",
    "\n",
    "*Report Submission Date: June 14, 2021*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/cf_schematic.png\" alt=\"collaborative-filtering-schematic\" title=\"Collaborative Filtering Method\" width=\"400\" height=\"400\"><br><center><i>Fig. 1: Collaborative Filtering recommends items based on how similar users liked an item <a id=\"ref4\" href=#4>[4]</a></i></center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [1 Introduction](#Introduction)\n",
    "    * [1.1 Objective](#Objective)\n",
    "    * [1.2 About Collaborative Filtering](#About-Collaborative-Filtering)\n",
    "    * [1.3 Working Principle & Assumptions](#Working-Principle-&-Assumptions)\n",
    "    * [1.4 Components](#Components)\n",
    "    * [1.5 Similarity Computation Criteria](#Similarity-Computation-Criteria)\n",
    "* [2 Main Problem: Data Sparsity](#Main-Problem:-Data-Sparsity)\n",
    "* [3 Previous Proposed Solutions & Drawbacks](#Previous-Proposed-Solutions-&-Drawbacks)\n",
    "* [4 Current Proposed Solution](#Current-Proposed-Solution)\n",
    "    * [4.1 Working Principle of EMDP](#Working-Principle-of-EMDP)\n",
    "    * [4.2 Similar Neighbors Selection by PCC](#Similar-Neighbors-Selection-by-PCC)\n",
    "    * [4.3 Significance Weighting](#Significance-Weighting)\n",
    "    * [4.4 Similarity Thresholds](#Similarity-Thresholds)\n",
    "    * [4.5 Missing Data Prediction](#Missing-Data-Prediction)\n",
    "    * [4.6 Prediction for Active Users](#Prediction-for-Active-Users)\n",
    "* [5 Implementation](#Implementation)\n",
    "    * [5.1 About MovieLens 100K Dataset](#About-MovieLens-100K-Dataset)\n",
    "    * [5.2 Data Import](#Data-Import)\n",
    "    * [5.3 Hold-Out Sampling](#Hold-Out-Sampling)\n",
    "    * [5.4 Matrix Conversion](#Matrix-Conversion)\n",
    "    * [5.5 Similarity Matrix Calculation](#Similarity-Matrix-Calculation)\n",
    "    * [5.6 Predict Missing Values](#Predict-Missing-Values)\n",
    "* [6 Evaluation](#Evaluation)\n",
    "    * [6.1 PCC Calculation](#PCC-Calculation)\n",
    "    * [6.2 Prediction on Test Set](#Prediction-on-Test-Set)\n",
    "    * [6.3 MAE and RMSE Computation](#MAE-and-RMSE-Computation)\n",
    "* [7 Impact of EMDP](#Impact-of-EMDP)\n",
    "* [8 Ending Notes](#Ending-Notes)\n",
    "* [9 References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Recommender Systems, k-nearest neighbor-based Collaborative Filtering (also called memory-based Collaborative Filtering) faces a common challenge of missing values, which data scientists and researchers have been working hard to solve and have proposed solutions in the published paper : _\"Effective Missing Data Prediction for Collaborative Filtering\"_ <a id=\"ref3\" href=#3>[3]</a>.\n",
    "\n",
    "Based on the solution proposed in the paper, in this report we discuss and implement a solution framework in Python for effective missing data prediction algorithm in Collaborative Filtering. This is performed without using any Recommender Systems-related libraries. Also, this report discusses how and why the solution provided is able to tackle the missing value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is one of the most popular and successful techniques used by recommender systems which automatically predicts or filters the interests of an active user by collecting or collaborating information about preferences or tastes from other similar users or items. These kinds of techniques are widely employed in several large and popular commercial systems, such as Amazon, Ebay, Netflix, Facebook, Spotify etc.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/netflix_recsys.png\" alt=\"netflix-recsys\" title=\"Netflix Recommendation System\" width=\"600\"><br><center><i>Fig. 2: An example of a personalized recommendation on Netflix homepage <a id=\"ref6\" href=#6>[6]</a></i></center>\n",
    "</p>\n",
    "\n",
    "In the above image, we can see an example of a personalized recommendation on Netflix homepage. Netflixâ€™s own recommendation system (termed as _The Netflix Experience_) is powered by a family of ranking algorithms, each optimized for a different purpose. For instance, the _Top Picks_ row on the homepage makes recommendations based on a personalized ranking of videos, and the _Trending Now_ row also incorporates recent popularity trends. The _Because You Watched_ row shows recommendations of similar movie/series based on user's watching history. These algorithms, along with many others, are used together to construct personalized home pages for over 100 million members.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/netflix_workflow.png\" alt=\"netflix-workflow\" title=\"Netflix Recommendation Workflow\" width=\"600\"><br><center><i>Fig. 3: Netflix Recommendation Workflow <a id=\"ref7\" href=#7>[7]</a></i></center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Principle & Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It builds on the idea of recommending an item to an active user depending on other like-minded users. The underlying assumption or hypothesis here is :\n",
    "\n",
    ">If person $A$ has the same opinion as person $B$ on an issue, $A$ is more likely to also have $B$'s opinion on a different issue than that of a randomly chosen person.\n",
    "\n",
    "In terms of user-item scenario, although users have their own rating style, in practice if an item is a very popular item and has obtained a very high average rating from other users, then the active user will have a high probability to give this item a good rating too.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/user_item_mat.png\" alt=\"user-item-matrix\" title=\"Matrix Factorization in Collaborative Filtering\" width=\"600\"><br><center><i>Fig. 4: Working of Collaborative Filtering Process: User-Item Matrix <a id=\"ref8\" href=#8>[8]</a></i></center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering systems typically contains a user-rating matrix that includes:\n",
    "\n",
    "- Set of users\n",
    "- Set of items\n",
    "- Set of opinions about items: ratings, reviews, purchases etc.\n",
    "\n",
    "It can be classified into two types: _Memory-based_ and _Model-based_.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/cf_types.png\" alt=\"cf-types\" title=\"Types of Collaborative Filtering\" width=\"700\"><br><center><i>Fig. 5: Examples of (a) Memory-based Collaborative Filtering and (b) Model-based Collaborative Filtering <a id=\"ref2\" href=#2>[2]</a></i></center>\n",
    "</p>\n",
    "\n",
    "_Memory-based methods_ include:\n",
    "-\tUser-based approach, which predicts the ratings of an active user based on the ratings of similar users having similar rating styles.\n",
    "-\tItem-based approach, which predicts the ratings of an active user based on the ratings of similar items for each item rated by the user.\n",
    "\n",
    "_Model-based methods_ include:\n",
    "-\tClustering models\n",
    "-\tAspect models\n",
    "-\tLatent factor models\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/memory_based_types.png\" alt=\"memory-based-types\" title=\"Types of Memory-based Collaborative Filtering\" width=\"700\"><br><center><i>Fig. 6: Examples of (a) User-based approach and (b) Item-based approach <a id=\"ref5\" href=#5>[5]</a></i></center>\n",
    "</p>\n",
    "\n",
    "However, model-based methods are generally time-consuming to build and update, and are unable to cover diverse users ranges like memory-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Computation Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable similarity computation algorithms include _Pearson Correlation Coefficient_ (PCC) and _Vector Space Similarity_ (VSS) algorithms. PCC-based collaborative filtering generally can achieve higher performance than the other popular algorithm VSS, since it considers the differences of user rating styles. Also, their implementation is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Problem: Data Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue with collaborative filtering is **Data Sparsity**, i.e., insufficient information on a user's rating history.\n",
    "User-item matrix of commercial recommendation system is generally very sparse and the density of available ratings is often less than 1%. This is because every user is practically able to rate a limited or only few items at the same time. This sparsity or missing data in the user-item matrix directly leads to the prediction inaccuracy in collaborative filtering, thereby producing inaccurate recommendation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Proposed Solutions & Drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the previously proposed solution and their drawbacks are :\n",
    "\n",
    "- _Cluster-based smoothing_: Introducing cluster-based smoothing methods to fill the missing values can improve the filtering performance. This clusters the users using K-means first, and then predicts all the missing data based on the ratings of Top-N most similar users in the similar clusters. But it has a few drawbacks as well :\n",
    "\n",
    " - The clustering algorithm limits the diversity of users in each cluster.\n",
    "\n",
    " - Clustering results of K-means relies on the preselected K users.\n",
    " \n",
    " - For an active user who hasn't rated enough items or doesn't have enough similar users, predicting all the missing data could bring negative influence for the recommendation.\n",
    " \n",
    "\n",
    "- _Dimensionality reduction_:  Dimensionality reduction approaches have been proposed to handle the data sparsity issue, which delete any unrelated or insignificant users or items. But this could discard some important information of the user-item matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Proposed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current proposed solution to tackle the data sparsity issue and therefore, increase density of user-item matrix is called _Effective Missing Data Prediction (EMDP)_ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Principle of EMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every missing data in the matrix is evaluated by utilizing the available information both from users and items (similar neighbors). Based on certain threshold values of confidence, missing data are selectively given prediction ratings. The thresholds decide whether missing data prediction will bring positive influence for the recommendation of active users :\n",
    "\n",
    "- If yes, then missing values are predicted by weighting of user and item similarities.\n",
    "- Otherwise, the missing data are set to zero. \n",
    "\n",
    "This consideration is different from all other existing methods of prediction which predict all missing data in the user-item matrix, giving rise to inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Neighbors Selection by PCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user and item similarities are calculated using Pearson Correlation Coefficient (PCC) as per following formulae :\n",
    "\n",
    "1. Similarity between users $a$ and $u$ :\n",
    "\n",
    "$$\\boxed{\\text{Sim}(a,u) = \\frac{\\sum_{i \\in I(a) \\cap I(u)}^{}{\\left( r_{a,i} - {\\overline{r}}_{a} \\right) \\cdot \\left( r_{u,i} - {\\overline{r}}_{u} \\right)}}{\\sqrt{\\sum_{i \\in I(a) \\cap I(u)}^{}\\left( r_{a,i} - {\\overline{r}}_{a} \\right)^{2}} \\cdot \\sqrt{\\sum_{i \\in I(a) \\cap I(u)}^{}\\left( r_{u,i} - {\\overline{r}}_{u} \\right)^{2}}}}$$\n",
    "\n",
    "where,<br> \n",
    "$r_{a,i} \\rightarrow$ rating user $a$ gave item $i$, and<br>\n",
    "${\\overline{r}}_{a} \\rightarrow$ averaging rate of user $a$.\n",
    "\n",
    "2. Similarity between items $i$ and $j$:\n",
    "\n",
    "$$\\boxed{\\text{Sim}(i,j) = \\frac{\\sum_{u \\in U(i) \\cap U(j)}^{}{(r_{u,i} - {\\overline{r}}_{i}) \\cdot (r_{u,j} - {\\overline{r}}_{j})}}{\\sqrt{\\sum_{u \\in U(i) \\cap U(j)}^{}\\left( r_{u,i} - {\\overline{r}}_{i} \\right)^{2}} \\cdot \\sqrt{\\sum_{u \\in U(i) \\cap U(j)}^{}\\left( r_{u,j} - {\\overline{r}}_{j} \\right)^{2}}}}$$\n",
    "\n",
    "where,<br>\n",
    "$r_{u,i} \\rightarrow$ rating user $u$ gave item $i$, and<br>\n",
    "${\\overline{r}}_{i} \\rightarrow$ average rating of item $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PCC tends to overestimate the similarities of users who happen to have rated a few items identically, but may not have similar overall preferences, a correlation significance weighting factor $\\gamma$ (gamma) is introduced. This factor would devalue similarity weights that were based on a small number of co-rated items.\n",
    "\n",
    "$$\\boxed{Sim'(a,u) = \\frac{Min(|I_{a} \\cap I_{u}\\ |,\\gamma)}{\\gamma} \\cdot Sim(a,u)}$$\n",
    "\n",
    "where $|I_{a} \\cap I_{u}\\ | \\rightarrow$ number of items which user $a$ and user $u$ rated in common.\n",
    "\n",
    "Likewise, for similarity between items a weighting factor $\\delta$ (delta) is introduced which would devalue similarity weights that were based on a small number of users who rated both items.\n",
    "\n",
    "$$\\boxed{Sim'(i,j) = \\frac{Min(|U_{i} \\cap U_{j}|,\\delta)}{\\delta} \\cdot Sim(i,j)}$$\n",
    "\n",
    "where $|U_{i} \\cap U_{j}| \\rightarrow$ number of users who rated both item $i$ and item $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, threshold values $\\eta$ and $\\theta$ are defined for user and item similarities, respectively. If the similarity between a user and its neighbor exceeds $\\eta$ , then this neighboring user is selected as being similar. Likewise, if the similarity between an item and its neighbor exceeds $\\theta$, then this neighboring item is selected as being similar. Therefore, for each missing data $r_{u,i}$ , two sets are generated:\n",
    "\n",
    "$$\\boxed{S(u) = \\ \\left\\{ u_{a} \\middle| Sim'(u_{a},\\ u)\\  > \\ \\eta,\\ u_{a} \\neq \\ u \\right\\}}$$\n",
    "\n",
    "where $u \\rightarrow$ active user and $u_{a} \\rightarrow$ similar user;\n",
    "\n",
    "$$\\boxed{S(i) = \\ \\left\\{ i_{k} \\middle| Sim'(i_{k},\\ i)\\  > \\ \\theta,\\ i_{k} \\neq \\ i \\right\\}}$$\n",
    "\n",
    "where $i \\rightarrow$ item and $i_{k} \\rightarrow$ similar item.\n",
    "\n",
    ">__NOTE__ : Thresholds $\\gamma$, $\\delta$, $\\eta$ and $\\theta$ need to be fine-tuned as:\n",
    ">- too high value causes shortage of similar users or items (highly sparse user-item matrix).\n",
    ">- too low value brings too many similar users or items causing overestimation (highly dense user-item matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user-based and item-based approaches are systematically combined in order to take advantage of user correlations and item correlations in the user-item matrix. This is done because in practice, predicting missing data only using user-based approaches or only using item-based approaches potentially ignores valuable information required to make more accurate prediction.\n",
    "\n",
    "Based on the sets $S(u)$ and $S(i)$ generated for each missing data $r_{u,i}$ the following four combination cases are possible and their respective predictions $P\\left( r_{u,i} \\right)$ are given as :\n",
    "\n",
    "Case I:\n",
    "User $u$ has similar users as well as item $i$ has similar items $\\Rightarrow S(u) \\neq \\varnothing \\land S(i) \\neq \\varnothing$ :\n",
    "\n",
    "$$\\boxed{P\\left( r_{u,i} \\right) = \\ \\lambda \\times \\left( \\overline{u}\\  + \\frac{\\sum_{u_{a} \\in S(u)}^{}{Sim'\\left( u_{a},\\ u \\right) \\cdot \\left( r_{u_{a},i} - {\\overline{u}}_{a} \\right)}}{\\sum_{u_{a} \\in S(u)}^{}{Sim'\\left( u_{a},\\ u \\right)}} \\right) + (1 - \\lambda) \\times \\left( \\overline{i}\\  + \\frac{\\sum_{i_{k} \\in S(i)}^{}{Sim'\\left( i_{k},\\ i \\right) \\cdot \\left( r_{u,i_{k}} - {\\overline{i}}_{k} \\right)}}{\\sum_{i_{k} \\in S(i)}^{}{Sim'\\left( i_{k},\\ i \\right)}} \\right)}$$\n",
    "\n",
    "Case II:\n",
    "User $u$ has similar users, but item $i$ doesn't have similar items $\\Rightarrow S(u) \\neq \\varnothing \\land S(i) = \\varnothing$ :\n",
    "\n",
    "$$\\boxed{P\\left( r_{u,i} \\right) = \\ \\overline{u}\\  + \\frac{\\sum_{u_{a} \\in S(u)}^{}{Sim'\\left( u_{a},\\ u \\right) \\cdot \\left( r_{u_{a},i} - {\\overline{u}}_{a} \\right)}}{\\sum_{u_{a} \\in S(u)}^{}{Sim'\\left( u_{a},\\ u \\right)}}}$$\n",
    "\n",
    "Case III:\n",
    "User $u$ doesn't have similar users, but item $i$ has similar items $\\Rightarrow S(u) = \\varnothing \\land S(i) \\neq \\varnothing$ :\n",
    "\n",
    "$$\\boxed{P\\left( r_{u,i} \\right) = \\ \\overline{i}\\  + \\frac{\\sum_{i_{k} \\in S(i)}^{}{Sim'\\left( i_{k},\\ i \\right) \\cdot \\left( r_{u,i_{k}} - {\\overline{i}}_{k} \\right)}}{\\sum_{i_{k} \\in S(i)}^{}{Sim'\\left( i_{k},\\ i \\right)}}}$$\n",
    "\n",
    "Case IV:\n",
    "User $u$ doesn't have similar users as well as item $i$ doesn't have similar items $\\Rightarrow S(u) = \\varnothing \\land S(i) = \\varnothing$ :\n",
    "\n",
    "$$\\boxed{P\\left( r_{u,i} \\right) = \\ 0}$$\n",
    "\n",
    "Here, $\\lambda$ parameter (introduced in Case I) is in the range $[0,1]$ and its value determines the weighting of prediction reliability, i.e., how closely the prediction relies on user-based and item-based approach. Case II and Case III are in fact special cases of Case I where :\n",
    "\n",
    "- $\\lambda = 1 \\rightarrow$ prediction depends completely on ratings from user-based approach\n",
    "- $\\lambda = 0 \\rightarrow$ prediction depends completely on ratings from item-based approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for Active Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the missing data prediction, the ratings for the active users are predicted in exactly similar fashion for the first three possible cases (equations 7,8,9). But for Case IV, i.e., if user $a$ doesn't have similar users as well as item $i$ doesn't have similar items $\\Rightarrow S(a) = \\varnothing \\land S(i) = \\varnothing$, prediction is done here differently as :\n",
    "\n",
    "$$\\boxed{P\\left( r_{a,i} \\right) = \\ \\lambda \\times {\\overline{r}}_{a} + (1 - \\lambda) \\times {\\overline{r}}_{i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About MovieLens 100K Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLens data sets were collected by the [GroupLens Research Project](https://grouplens.org/) at the University of Minnesota. They operate the [MovieLens](http://www.movielens.org/) web site which is a movie recommender based on collaborative filtering, helping people find movies to watch and has hundreds of thousands of registered users. GroupLens conducts online field experiments in MovieLens in the areas of automated content recommendation, recommendation interfaces, tagging-based recommenders and interfaces, member-maintained databases, and intelligent user interface design.\n",
    "\n",
    "Specifically, the MovieLens 100K dataset <a id=\"ref1\" href=#1>[1]</a> was collected during the seven-month period from September 19th, 1997 through April 22nd, 1998. This rating dataset has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. Detailed descriptions of the data file can be found here in this [README file](https://files.grouplens.org/datasets/movielens/ml-100k-README.txt).\n",
    "\n",
    "We will be using `u.data` dataset located inside the zipped folder for MovieLens 100K, that consists of the following :\n",
    "\n",
    "- 100,000 ratings (1-5) by 943 users on 1682 items (movies).\n",
    "- Each user has rated at least 20 movies.\n",
    "- Users and items are numbered consecutively from 1. \n",
    "- The data is randomly ordered. \n",
    "- This is a tab separated list of user id | item id | rating | timestamp. \n",
    "- The time stamps are unix seconds since 1/1/1970 UTC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import necessary `pandas` and `numpy` packages, and then load the MovieLens 100K dataset into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0mG3n0qRNTSA",
    "outputId": "cb0eadbd-b8a8-41b1-e3b4-7f44b47a8804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('data/ml-100k/u.data', sep='\\t', names=names)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density of the user-item matrix is calculated to be $6.30\\%$, which is fairly sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Percentage: 6.30%\n"
     ]
    }
   ],
   "source": [
    "n_ratings = df.shape[0]\n",
    "n_users = df.user_id.nunique()\n",
    "n_items = df.item_id.nunique()\n",
    "den_perc = n_ratings/(n_users*n_items)*100\n",
    "print(f\"Density Percentage: {den_perc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief summary of relevant statistics shows that the number of ratings made by users is $106.04$ on average and in the interval of $[20,737]$. Whereas, the number of ratings for items is $59.45$ on average and in the interval of $[1,583]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of User Ratings:\n",
      "          rating\n",
      "max   737.000000\n",
      "min    20.000000\n",
      "mean  106.044539\n",
      "\n",
      "No. of Item Ratings:\n",
      "          rating\n",
      "max   583.000000\n",
      "min     1.000000\n",
      "mean   59.453032\n"
     ]
    }
   ],
   "source": [
    "stats = [\"max\", \"min\",\"mean\"]\n",
    "\n",
    "print(f\"No. of User Ratings:\\n{df.pivot_table(index = ['user_id'], values = ['rating'], aggfunc = 'count').describe().loc[stats]}\\n\")\n",
    "\n",
    "print(f\"No. of Item Ratings:\\n{df.pivot_table(index = ['item_id'], values = ['rating'], aggfunc = 'count').describe().loc[stats]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select $500$ each of most active users and most active items from the dataset, save those in a new dataframe `df_new` to be worked with further. This new dataframe now consists of $64957$ records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hFJT0QWudp1P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64957, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186      302       3  891717742\n",
       "3      244       51       2  880606923\n",
       "5      298      474       4  884182806\n",
       "6      115      265       2  881171488\n",
       "7      253      465       5  891628467"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_most_active_users = 500\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(n_most_active_users).index\n",
    "\n",
    "n_most_active_items = 500\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(n_most_active_items).index\n",
    "\n",
    "df_new = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]\n",
    "print(df_new.shape)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we map the internal IDs for the items to new IDs in serial order, by creating a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r012fu0jJkJc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186        0       3  891717742\n",
       "3      244        1       2  880606923\n",
       "5      298        2       4  884182806\n",
       "6      115        3       2  881171488\n",
       "7      253        4       5  891628467"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_ids = df_new['item_id'].unique().tolist()\n",
    "\n",
    "item_dict = dict(zip(i_ids, [i for i in range(len(i_ids))]))\n",
    "\n",
    "df_new = df_new.copy()\n",
    "df_new['item_id'] = df_new['item_id'].map(item_dict)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ3rlC7jO6WJ"
   },
   "source": [
    "### Hold-Out Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the number of training users and active users to be 60% and 40% of the most active users, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training users: 300, No. of active users: 200\n"
     ]
    }
   ],
   "source": [
    "n_training_users = int(0.6*n_most_active_users)\n",
    "n_active_users = int(0.4*n_most_active_users)\n",
    "print(f\"No. of training users: {n_training_users}, No. of active users: {n_active_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the number of given ratings for active users to $20$  as per the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIVEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly select users from the most active users as the training set `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62</td>\n",
       "      <td>257</td>\n",
       "      <td>2</td>\n",
       "      <td>879372434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating  timestamp\n",
       "0       196      242       3  881250949\n",
       "2        22      377       1  878887116\n",
       "3       244       51       2  880606923\n",
       "7       253      465       5  891628467\n",
       "10       62      257       2  879372434"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_uids = np.random.choice(df.user_id.unique(), \n",
    "                               n_training_users, \n",
    "                               replace=False)\n",
    "\n",
    "train_df = df[df['user_id'].isin(random_uids)]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then map the new internal IDs for all users in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>257</td>\n",
       "      <td>2</td>\n",
       "      <td>879372434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating  timestamp\n",
       "0         0      242       3  881250949\n",
       "2         1      377       1  878887116\n",
       "3         2       51       2  880606923\n",
       "7         3      465       5  891628467\n",
       "10        4      257       2  879372434"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ids = train_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "train_df = train_df.copy()\n",
    "train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of users are active users for testing, which are assigned to `remain_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>886324817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186      302       3  891717742\n",
       "4      166      346       1  886397596\n",
       "5      298      474       4  884182806\n",
       "6      115      265       2  881171488\n",
       "8      305      451       3  886324817"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remain_df = df[~df['user_id'].isin(random_uids)]\n",
    "remain_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now map the new internal IDs for all these active users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>886324817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1        0      302       3  891717742\n",
       "4        1      346       1  886397596\n",
       "5        2      474       4  884182806\n",
       "6        3      265       2  881171488\n",
       "8        4      451       3  886324817"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ids = remain_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "remain_df = remain_df.copy()\n",
    "remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n",
    "remain_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these active users, we then randomly select `GIVEN` 20 ratings for the active users and assign them to `active_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwS00lvHO-ca",
    "outputId": "f9f3a031-b4e7-439e-c3e3-c165b2286d19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>879023115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>3</td>\n",
       "      <td>879023571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56711</th>\n",
       "      <td>0</td>\n",
       "      <td>1033</td>\n",
       "      <td>3</td>\n",
       "      <td>879024212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>4</td>\n",
       "      <td>879023390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11138</th>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>1</td>\n",
       "      <td>879022858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating  timestamp\n",
       "14473        0      100       4  879023115\n",
       "1294         0      263       3  879023571\n",
       "56711        0     1033       3  879024212\n",
       "2309         0      281       4  879023390\n",
       "11138        0      288       1  879022858"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_df = remain_df.groupby('user_id').sample(n=GIVEN, random_state=1024)\n",
    "active_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `remain_df` dataframe, we then select rest of the active users and assign them to `test_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>886324817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>883603013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>222</td>\n",
       "      <td>5</td>\n",
       "      <td>876042340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating  timestamp\n",
       "5         2      474       4  884182806\n",
       "6         3      265       2  881171488\n",
       "8         4      451       3  886324817\n",
       "9         5       86       3  883603013\n",
       "12        6      222       5  876042340"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = remain_df[~remain_df.index.isin(active_df.index)]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the format of the training dataset to matrices. \n",
    "\n",
    "We do this by left-merging a dataframe `df_zeros_train` that contains all combinations of $300$ training user IDs ($0$ to $299$) and all $500$ item IDs ($0$ to $499$) with the `train_df` dataframe, joined on the columns containing user IDs and item IDs. Also we fill any resulting `NA` values with zeros. Finally, we convert it to a pivot table having the user IDs as rows and item IDs as columns. This becomes the training matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix:\n",
      "item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
      "user_id                                                    ...                  \n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0  ...  0.0  0.0  0.0   \n",
      "1        0.0  0.0  2.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2        0.0  4.0  0.0  5.0  0.0  0.0  0.0  4.0  0.0  5.0  ...  0.0  0.0  0.0   \n",
      "3        0.0  5.0  0.0  0.0  4.0  0.0  0.0  0.0  4.0  0.0  ...  5.0  0.0  0.0   \n",
      "4        0.0  2.0  0.0  3.0  4.0  0.0  0.0  4.0  5.0  4.0  ...  0.0  0.0  0.0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "295      0.0  2.0  4.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0   \n",
      "296      0.0  4.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  3.0  ...  0.0  0.0  0.0   \n",
      "297      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "298      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  ...  0.0  0.0  0.0   \n",
      "299      0.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "item_id  493  494  495  496  497  498  499  \n",
      "user_id                                     \n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3        0.0  5.0  0.0  5.0  0.0  0.0  0.0  \n",
      "4        0.0  0.0  0.0  0.0  0.0  4.0  0.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  \n",
      "295      0.0  0.0  4.0  0.0  0.0  3.0  0.0  \n",
      "296      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "297      0.0  0.0  0.0  5.0  0.0  5.0  0.0  \n",
      "298      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "299      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[300 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "df_zeros_train = pd.DataFrame({'user_id': np.tile(np.arange(0, n_training_users),\n",
    "                                                  n_most_active_items), \n",
    "                               'item_id': np.repeat(np.arange(0, n_most_active_items),\n",
    "                                                    n_training_users), \n",
    "                               'rating': 0})\n",
    "\n",
    "train_ds = df_zeros_train.merge(train_df,\n",
    "                                how = 'left',\n",
    "                                on = ['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', \n",
    "                                                                                    index='user_id', \n",
    "                                                                                    columns='item_id')\n",
    "\n",
    "print(f\"Training Matrix:\\n{train_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way, we convert the `active_df` and `test_df` datasets to matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-ke62G3jiYb",
    "outputId": "1c135984-edb4-4f2b-fc73-9225d86a0c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Users Matrix:\n",
      "item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
      "user_id                                                    ...                  \n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "195      0.0  4.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n",
      "196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "197      0.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "item_id  493  494  495  496  497  498  499  \n",
      "user_id                                     \n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  \n",
      "195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[200 rows x 500 columns]\n",
      "\n",
      "\n",
      "Testing Matrix:\n",
      "item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
      "user_id                                                    ...                  \n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2        0.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  4.0  ...  0.0  0.0  0.0   \n",
      "3        0.0  0.0  0.0  0.0  4.0  0.0  0.0  5.0  5.0  5.0  ...  0.0  0.0  0.0   \n",
      "4        0.0  5.0  2.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  ...  0.0  0.0  0.0   \n",
      "198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "item_id  493  494  495  496  497  498  499  \n",
      "user_id                                     \n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2        0.0  0.0  0.0  5.0  0.0  5.0  0.0  \n",
      "3        0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  \n",
      "195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[200 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "df_zeros_test = pd.DataFrame({'user_id': np.tile(np.arange(0, n_active_users), \n",
    "                                                 n_most_active_items), \n",
    "                              'item_id': np.repeat(np.arange(0, n_most_active_items), \n",
    "                                                   n_active_users), \n",
    "                              'rating': 0})\n",
    "\n",
    "active_ds = df_zeros_test.merge(active_df, \n",
    "                                how = 'left', \n",
    "                                on = ['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', \n",
    "                                                                                    index='user_id', \n",
    "                                                                                    columns='item_id')\n",
    "test_ds = df_zeros_test.merge(test_df, \n",
    "                              how = 'left', \n",
    "                              on = ['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', \n",
    "                                                                                  index='user_id', \n",
    "                                                                                  columns='item_id')\n",
    "\n",
    "print(f\"Active Users Matrix:\\n{active_ds}\\n\")\n",
    "print(f\"\\nTesting Matrix:\\n{test_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the training matrix `train_ds` to a `NumPy` array. This is the ___User-Item Rating Matrix___ and the missing values are represented by the zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yElYv2TDKKGu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_train_ds = train_ds.values.copy()\n",
    "type(imputed_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the given proposition, the following parameters are required :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 0.7    # Î»\n",
    "GAMMA = 10      # Î³\n",
    "DELTA = 10      # Î´\n",
    "ETA = 0.7       # Î·\n",
    "THETA = 0.7     # Î¸\n",
    "EPSILON = 1e-9  # Îµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the number of users (rows) and items (columns) in the user-item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = imputed_train_ds.shape[0]\n",
    "n_items = imputed_train_ds.shape[1]\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first we calculate the ___User-User Similarity Matrix___ where each cell contains the similarity of ratings for every pair of users in the training dataset. Here, we make use of masks for filtering only the ratings co-rated by current pair of users $u$ and $u_a$. No co-rated ratings are skipped here. A small value of $\\text{EPSILON}\\ (\\epsilon)=10^{-9}$ is added to fraction denominators to prevent any undefined value occurring due to denominator being $0$. From the calculated average ratings of user pairs, we then calculate the user-user similarities(correlations) using Pearson Correlation Coefficients and apply significance weighting of $\\text{GAMMA}\\ (\\gamma)=10$. These values are then save to a new matrix `user_corr`, the row and column indexes of which denote the user IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zYh1bVd0ncz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-User Similarities :\n",
      "           0         1         2         3         4         5         6    \\\n",
      "0    1.000000  0.293778  0.145513 -0.313041  0.233893 -0.018490  0.598709   \n",
      "1    0.293778  1.000000  0.224561  0.126497  0.414504  0.257308  0.276510   \n",
      "2    0.145513  0.224561  1.000000 -0.198550  0.300494  0.066397  0.064681   \n",
      "3   -0.313041  0.126497 -0.198550  1.000000  0.092775 -0.013063  0.130789   \n",
      "4    0.233893  0.414504  0.300494  0.092775  1.000000  0.061269  0.098989   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "295  0.028211  0.094499  0.127791 -0.269807  0.210176 -0.141170  0.149057   \n",
      "296 -0.186404  0.628405  0.173766  0.069943  0.125097  0.038426  0.345880   \n",
      "297  0.100000  0.400256  0.044220  0.172087  0.025602 -0.146467  0.244937   \n",
      "298  0.270739  0.378360  0.347701  0.043618  0.653566 -0.192064  0.283573   \n",
      "299 -0.369539 -0.050821  0.122013  0.632703 -0.001894  0.105897 -0.082844   \n",
      "\n",
      "          7         8         9    ...       290       291       292  \\\n",
      "0    0.421054 -0.540485 -0.136773  ... -0.152677 -0.405868  0.223383   \n",
      "1   -0.166548  0.411678  0.547316  ... -0.161281  0.141056 -0.190927   \n",
      "2    0.167342 -0.055162  0.399846  ... -0.500719 -0.049912  0.385666   \n",
      "3   -0.094316  0.337328  0.008705  ...  0.100000  0.065793 -0.403607   \n",
      "4   -0.322980  0.215109  0.396142  ... -0.198262  0.240281 -0.171509   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "295 -0.274403  0.239273 -0.030496  ...  0.096922  0.191105  0.322116   \n",
      "296  0.332515  0.155419  0.381408  ... -0.152677  0.356016 -0.009799   \n",
      "297 -0.007634  0.119959  0.052026  ...  0.189573 -0.351370  0.314965   \n",
      "298 -0.292654  0.019440  0.404165  ...  0.000000 -0.209496  0.209410   \n",
      "299 -0.278050  0.206037 -0.139398  ...  0.000000  0.100901  0.195788   \n",
      "\n",
      "          293       294       295       296       297       298       299  \n",
      "0   -0.165890 -0.147853  0.028211 -0.186404  0.100000  0.270739 -0.369539  \n",
      "1    0.199335  0.100000  0.094499  0.628405  0.400256  0.378360 -0.050821  \n",
      "2    0.404261 -0.085159  0.127791  0.173766  0.044220  0.347701  0.122013  \n",
      "3    0.152353  0.085159 -0.269807  0.069943  0.172087  0.043618  0.632703  \n",
      "4    0.246253  0.302553  0.210176  0.125097  0.025602  0.653566 -0.001894  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "295  0.180252 -0.163570  1.000000 -0.178487  0.490085  0.616251 -0.191319  \n",
      "296  0.395372  0.353645 -0.178487  1.000000  0.084053  0.071310  0.176306  \n",
      "297  0.131874  0.054464  0.490085  0.084053  1.000000  0.261857 -0.173171  \n",
      "298  0.418535  0.584657  0.616251  0.071310  0.261857  1.000000  0.387567  \n",
      "299  0.269970  0.391420 -0.191319  0.176306 -0.173171  0.387567  1.000000  \n",
      "\n",
      "[300 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "user_corr = np.zeros((n_users, n_users)) \n",
    "\n",
    "for x, u in enumerate(imputed_train_ds):\n",
    "    for y, ua in enumerate(imputed_train_ds):\n",
    "        \n",
    "        # ratings co-rated by current pair of users\n",
    "        mask_x = u > 0\n",
    "        mask_y = ua > 0\n",
    "        \n",
    "        # co-rated item index\n",
    "        cor_index = np.intersect1d(np.where(mask_x), np.where(mask_y))\n",
    "        \n",
    "        # skip for no co-rated ratings\n",
    "        if len(cor_index) == 0:\n",
    "            continue\n",
    "        \n",
    "        # average ratings of current user pair\n",
    "        u_mean = np.sum(u)/(np.sum(np.clip(u, 0, 1)) + EPSILON)\n",
    "        ua_mean = np.sum(ua)/(np.sum(np.clip(ua, 0, 1)) + EPSILON)\n",
    "        \n",
    "        # user similarity using pearson correlation coefficient\n",
    "        u_sum_sqrt = np.sqrt(np.sum(np.square(u[cor_index] - u_mean)))\n",
    "        ua_sum_sqrt = np.sqrt(np.sum(np.square(ua[cor_index] - ua_mean)))\n",
    "        sim = np.sum((u[cor_index] - u_mean) * (ua[cor_index] - ua_mean)) / (u_sum_sqrt * ua_sum_sqrt + EPSILON)\n",
    "        \n",
    "        # significance-weighted user similarity\n",
    "        weighted_sim = (min(len(cor_index), GAMMA) / GAMMA) * sim\n",
    "        \n",
    "        # impute value into matrix \n",
    "        user_corr[x][y] = weighted_sim\n",
    "        \n",
    "print(\"User-User Similarities :\\n\", pd.DataFrame(user_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we calculate the ___Item-Item Similarity Matrix___ in a similar way, where each cell contains the similarity of ratings for every pair of items. Here, the row and column indexes denote the item IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-Item Similarities :\n",
      "      0         1         2         3         4         5         6    \\\n",
      "0    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1    0.0  1.000000  0.348008  0.217095 -0.032689  0.498341  0.060402   \n",
      "2    0.0  0.348008  1.000000  0.016143  0.313480 -0.256172  0.100000   \n",
      "3    0.0  0.217095  0.016143  1.000000 -0.256399  0.461854  0.000000   \n",
      "4    0.0 -0.032689  0.313480 -0.256399  1.000000 -0.607518  0.190952   \n",
      "..   ...       ...       ...       ...       ...       ...       ...   \n",
      "495  0.0  0.447878  0.431988  0.200000 -0.134365  0.000000  0.100000   \n",
      "496  0.0  0.418709  0.420232 -0.337227  0.172359 -0.067145 -0.100000   \n",
      "497  0.0 -0.187471 -0.232230 -0.146054  0.255970 -0.200000 -0.100000   \n",
      "498  0.0  0.350564 -0.070467 -0.085492 -0.208899  0.100000  0.194028   \n",
      "499  0.0  0.185521 -0.106928  0.173623  0.051900 -0.088880 -0.100000   \n",
      "\n",
      "          7         8         9    ...       490       491       492  \\\n",
      "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "1    0.131312  0.280164 -0.072136  ...  0.366525  0.272078 -0.074932   \n",
      "2    0.134708  0.578638 -0.060810  ... -0.100000  0.556732 -0.024884   \n",
      "3    0.243706  0.306095  0.474473  ...  0.030443  0.234102 -0.256376   \n",
      "4    0.094215  0.375551  0.045044  ...  0.472906 -0.036898 -0.126685   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "495  0.239722  0.435138 -0.103759  ...  0.252740  0.466775 -0.341148   \n",
      "496 -0.033168  0.349860  0.195813  ...  0.382433  0.341437  0.179979   \n",
      "497  0.484165  0.107403  0.540836  ...  0.420346  0.113881  0.342414   \n",
      "498 -0.409470 -0.074105  0.306037  ...  0.195661 -0.065629  0.392023   \n",
      "499  0.253261  0.312868  0.412953  ...  0.468083  0.487885 -0.302338   \n",
      "\n",
      "          493       494       495       496       497       498       499  \n",
      "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1    0.364103 -0.175680  0.447878  0.418709 -0.187471  0.350564  0.185521  \n",
      "2   -0.249297  0.094072  0.431988  0.420232 -0.232230 -0.070467 -0.106928  \n",
      "3   -0.100000 -0.100000  0.200000 -0.337227 -0.146054 -0.085492  0.173623  \n",
      "4   -0.341710  0.408982 -0.134365  0.172359  0.255970 -0.208899  0.051900  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "495  0.035203 -0.015535  1.000000  0.054903  0.221721 -0.075288  0.525856  \n",
      "496  0.032346  0.155507  0.054903  1.000000  0.133653  0.238121  0.156776  \n",
      "497  0.157078  0.494673  0.221721  0.133653  1.000000 -0.103232  0.501600  \n",
      "498  0.715459 -0.158970 -0.075288  0.238121 -0.103232  1.000000 -0.056250  \n",
      "499 -0.116767  0.275512  0.525856  0.156776  0.501600 -0.056250  1.000000  \n",
      "\n",
      "[500 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "item_corr = np.zeros((n_items, n_items))\n",
    "\n",
    "for x, i in enumerate(imputed_train_ds.T):\n",
    "    for y, ik in enumerate(imputed_train_ds.T):\n",
    "        \n",
    "        # ratings co-rated by current pair of items\n",
    "        mask_x = i > 0\n",
    "        mask_y = ik > 0\n",
    "        \n",
    "        # co-rated user index\n",
    "        cor_index = np.intersect1d(np.where(mask_x), np.where(mask_y))\n",
    "        \n",
    "        # skip for no co-rated ratings\n",
    "        if len(cor_index) == 0:\n",
    "            continue\n",
    "            \n",
    "        # average ratings of current item pair\n",
    "        i_mean = np.sum(i)/(np.sum(np.clip(i, 0, 1)) + EPSILON)\n",
    "        ik_mean = np.sum(ik)/(np.sum(np.clip(ik, 0, 1)) + EPSILON)\n",
    "        \n",
    "        # item similarity using pearson correlation coefficient\n",
    "        i_sum_sqrt = np.sqrt(np.sum(np.square(i[cor_index] - i_mean)))\n",
    "        ik_sum_sqrt = np.sqrt(np.sum(np.square(ik[cor_index] - ik_mean)))\n",
    "        sim = np.sum((i[cor_index] - i_mean) * (ik[cor_index] - ik_mean)) / (i_sum_sqrt * ik_sum_sqrt + EPSILON)\n",
    "\n",
    "        # significance-weighted item similarity\n",
    "        weighted_sim = (min(len(cor_index), DELTA) / DELTA) * sim\n",
    "        \n",
    "        # impute value into matrix\n",
    "        item_corr[x][y] = weighted_sim\n",
    "        \n",
    "print(\"Item-Item Similarities :\\n\", pd.DataFrame(item_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy4FurbHD4dt"
   },
   "source": [
    "### Predict Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we predict all missing data in the training matrix `imputed_train_ds` by using the proposed solution framework. \n",
    "\n",
    "Here, we iterate through each cell of the training matrix with missing rating to generate similarity sets of users and items selected on the basis of threshold values $\\text{ETA}\\ (\\eta)=0.7$ and $\\text{THETA}\\ (\\theta)=0.7$ respectively. After that, we follow the formula discussed in the above sections and perform predictions based on similar user and similar item sets.\n",
    "\n",
    "Using the previously constructed similarity matrices : `user_corr` and `item_corr`, we predict the missing data for 4 possible combinations of user and item sets, as discussed above in [section 4.5](#Missing-Data-Prediction). For missing values having null similar user and item set, we set them to zero which actually permits selective missing data prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (u, i), rating in np.ndenumerate(imputed_train_ds):\n",
    "    if (rating == 0):\n",
    "                 \n",
    "        # index set of similar users for current user excluding itself, based on ETA threshold\n",
    "        idx, = np.where(user_corr[u] > ETA)\n",
    "        sim_user_ids = idx[np.argsort(user_corr[u][idx])[:-1]]\n",
    "        \n",
    "        # index set of similar items for current item excluding itself, based on THETA threshold\n",
    "        idy, = np.where(item_corr[i] > THETA)\n",
    "        sim_item_ids = idy[np.argsort(item_corr[i][idy])[:-1]]\n",
    "                                    \n",
    "        # similarity set of users\n",
    "        sim_user_val = user_corr[u][sim_user_ids]\n",
    "        \n",
    "        # similarity set of items\n",
    "        sim_item_val = item_corr[i][sim_item_ids]\n",
    "      \n",
    "        # average of current user's ratings\n",
    "        user_mean = np.sum(imputed_train_ds[u]) / (np.sum(np.clip(imputed_train_ds[u], 0, 1)) + EPSILON)\n",
    "        sim_users = imputed_train_ds[sim_user_ids]\n",
    "        sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)                   \n",
    "        \n",
    "        # average of current item's ratings\n",
    "        item_mean = np.sum(imputed_train_ds.T[i]) / (np.sum(np.clip(imputed_train_ds.T[i], 0, 1)) + EPSILON)\n",
    "        sim_items = imputed_train_ds.T[sim_item_ids]\n",
    "        sim_item_mean = np.sum(sim_items, axis=1) / (np.sum(np.clip(sim_items, 0, 1), axis=1) + EPSILON)                               \n",
    "                                \n",
    "        # select users who rated item i\n",
    "        mask_rated_i = sim_users[:, i] > 0\n",
    "        \n",
    "        # sim'(ua, u) * (r_ua,i - mean_ua)\n",
    "        sim_user_sum_mean = sim_user_val[mask_rated_i] * (sim_users[mask_rated_i, i] - sim_user_mean[mask_rated_i]) \n",
    "       \n",
    "        # sim'(ik, i) * (r_u,ik - mean_ik)\n",
    "        sim_item_sum_mean = sim_item_val * (sim_items[:, u] - sim_item_mean)\n",
    "        \n",
    "        # filter unrated items\n",
    "        w = np.clip(sim_items[:, u], 0, 1)\n",
    "        sim_item_sum_mean *= w\n",
    "        \n",
    "        # prediction based on similar user and similar item sets\n",
    "        if (sim_user_val.size and sim_item_val.size):\n",
    "            pred = LAMBDA*( user_mean + np.sum(sim_user_sum_mean) / (np.sum(sim_user_val[mask_rated_i]) + EPSILON) ) + \\\n",
    "                       (1-LAMBDA)*( item_mean + np.sum(sim_item_sum_mean) / (np.sum(sim_item_val * w) + EPSILON) )\n",
    "        \n",
    "        elif (sim_user_val.size and not sim_item_val.size):\n",
    "            pred = user_mean + np.sum(sim_user_sum_mean) / (np.sum(sim_user_val[mask_rated_i]) + EPSILON)\n",
    "        \n",
    "        elif (not sim_user_val.size and sim_item_val.size):\n",
    "            pred = item_mean + np.sum(sim_item_sum_mean) / (np.sum(sim_item_val * w) + EPSILON)\n",
    "        \n",
    "        elif (not sim_user_val.size and not sim_item_val.size):\n",
    "            pred = 0\n",
    "            \n",
    "        # impute predicted value to user-item matrix\n",
    "        imputed_train_ds[u][i] = np.clip(pred, 0, 5)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbOfVWTV_Aij"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the predictions,  we first convert the `imputed_train_ds` which is in `NumPy` array format back to `Pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "KoOgX_axKKGw",
    "outputId": "37a7adbd-e0d6-4375-e28c-3940977896f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_train_ds = pd.DataFrame(imputed_train_ds)\n",
    "type(imputed_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCC Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute Pearson Correlation Coefficients of all pairs of items between active set and imputed training set. A similar approach as we did before for calculating the similarity matrices is followed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq0uq1aHzu11",
    "outputId": "f6214a46-d63e-4fdc-e7dd-ccaed23b237d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07337825, -0.00273499, -0.09900332, ...,  0.35929164,\n",
       "         0.14237004,  0.13589609],\n",
       "       [-0.32552554,  0.2375235 ,  0.03698918, ..., -0.14601107,\n",
       "         0.05969067,  0.55183409],\n",
       "       [ 0.07820159, -0.04963948,  0.11441587, ...,  0.3331637 ,\n",
       "         0.28004406,  0.12890423],\n",
       "       ...,\n",
       "       [ 0.23214672,  0.48017719,  0.08289009, ...,  0.50221074,\n",
       "         0.32219139,  0.12699871],\n",
       "       [-0.24529062, -0.06940567,  0.17174559, ..., -0.16760629,\n",
       "        -0.09697383,  0.27494276],\n",
       "       [ 0.1591988 , -0.19660292, -0.035975  , ...,  0.09943822,\n",
       "        -0.23944152, -0.23964303]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_user_pearson_corr = np.zeros((active_ds.shape[0], train_ds.shape[0]))\n",
    "\n",
    "for i, user_i_vec in enumerate(active_ds.values):\n",
    "    for j, user_j_vec in enumerate(imputed_train_ds.values):\n",
    "        \n",
    "        # ratings corated by the current pair of users\n",
    "        mask_i = user_i_vec > 0\n",
    "        mask_j = user_j_vec > 0\n",
    "\n",
    "        # corrated item index, skip if no corrated ratings\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            continue\n",
    "\n",
    "        # average value of user_i_vec and user_j_vec\n",
    "        mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "        mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "        # compute PCC\n",
    "        user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "        user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "        r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "        r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "        r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "        r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "        sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "\n",
    "        # significance weighting\n",
    "        weighted_sim = (min(len(corrated_index), GAMMA) / GAMMA) * sim\n",
    "\n",
    "        active_user_pearson_corr[i][j] = weighted_sim\n",
    "\n",
    "active_user_pearson_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewTnN9kNb8Ys"
   },
   "source": [
    "### Prediction on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we predict the ratings on the test set. For this, we iterate through the testing matrix `test_ds` for all ratings greater than zero. Then, we the find mean active user ratings, mean similar user ratings, and coefficients of similar users (using the PCCs calculated in previous section). We then proceed to calculate the user-based predictions using equation no. 8 in [section 4.5](#Missing-Data-Prediction) : \n",
    "\n",
    "$P\\left( r_{u,i} \\right) = \\ \\overline{u}\\  + \\frac{\\sum_{u_{a} \\in S(u)}^{}{Sim'\\left( u_{a},\\ u \\right) \\cdot \\left( r_{u_{a},i} - {\\overline{u}}_{a} \\right)}}{\\sum_{u_{a} \\in S(u)}^{}{Sim'\\left( u_{a},\\ u \\right)}}$.\n",
    "\n",
    "Finally, we clip or limit these predicted values to interval $[1,5]$ since there are only $1$ to $5$ ratings available as per the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4ERndYXb8Ys",
    "outputId": "12607670-af61-403a-e4ce-f5e874bf8386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 4.11197723, 0.        , ..., 0.        , 4.22378082,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "test_ds_pred = np.zeros_like(test_ds.values)\n",
    "\n",
    "for (i, j), rating in np.ndenumerate(test_ds.values):\n",
    "\n",
    "    if rating > 0:\n",
    "\n",
    "        sim_user_ids = np.argsort(active_user_pearson_corr[i])[-1:-(K + 1):-1]\n",
    "\n",
    "        #==================user-based==================#\n",
    "        \n",
    "        # average value of active user's ratings\n",
    "        user_mean = np.sum(active_ds.values[i]) / (np.sum(np.clip(active_ds.values[i], 0, 1)) + EPSILON)   \n",
    "        \n",
    "        # similar user's ratings\n",
    "        sim_users = imputed_train_ds.values[sim_user_ids]\n",
    "        \n",
    "        # average value of similar user's ratings\n",
    "        sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)\n",
    "        \n",
    "        \n",
    "        # coefficient values of similar users\n",
    "        sim_val = active_user_pearson_corr[i][sim_user_ids]\n",
    "        \n",
    "        # select the users who rated item j\n",
    "        mask_rated_j = sim_users[:, j] > 0\n",
    "        \n",
    "       \n",
    "        # prediction = mean(u) + sum[sim(v,u) * (r_{v,j} - mean(v))] / sum[sim(v,u)]        \n",
    "        user_based_pred = user_mean + \\\n",
    "                          np.sum(sim_val[mask_rated_j]*(sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])) / \\\n",
    "                          (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "        \n",
    "        user_based_pred = np.clip(user_based_pred, 1, 5)\n",
    "\n",
    "        test_ds_pred[i][j] = user_based_pred\n",
    "        \n",
    "test_ds_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUTn4kSFb8ZA"
   },
   "source": [
    "### MAE and RMSE Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we calculate the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) values to compare the predictions on the test set with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JCVmexyb8ZA",
    "outputId": "6f650170-e9a6-482a-b695-5fa474d964b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7956406973413431\n",
      "RMSE: 1.0200527938518875\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "MAE = np.sum(np.abs(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1))\n",
    "\n",
    "# RMSE\n",
    "RMSE = np.sqrt(np.sum(np.square(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1)))\n",
    "\n",
    "print(f\"MAE: {MAE}\\nRMSE: {RMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of EMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The algorithm discussed incorporates the option of not predicting missing data if it doesn't meet the criteria set using threshold values. \n",
    "\n",
    "- Further, it removes potential negative influences from inaccurate prediction on the missing data. \n",
    "\n",
    "- The effectiveness of this approach is investigated in the given paper using simulations on a dataset and plotting Mean Absolute Error vs. $\\lambda$.\n",
    "\n",
    "- Mean Absolute Error (MAE) metrics is used here to measure the prediction quality of our proposed approach with other collaborative filtering methods, which is given by:\n",
    "\n",
    "$$\\text{MAE}=\\frac{\\sum_{u,i}\\left | r_{u,i}-\\hat{r}_{u,i} \\right |}{N}$$\n",
    "where<br>\n",
    "$r_{u,i} \\rightarrow$ actual rating that user $u$ gave to item $i$,<br>\n",
    "$\\hat{r}_{u,i} \\rightarrow$ predicted rating that user $u$ gave to item $i$,<br>\n",
    "$N \\rightarrow$ number of tested ratings.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"images/mae_comparison.png\" alt=\"mae_comparison\" title=\"MAE Comparison\" width=\"400\"><br><center><i>Fig. 7: MAE Comparison of EMDP vs PEMD <a id=\"ref3\" href=#3>[3]</a></i></center>\n",
    "</p>\n",
    "\n",
    "Consequently, this proposed method's (EMDP) performance is compared with that of an approach which predicts every missing data (PEMD). The results show that this selective missing-data prediction algorithm has lower MAE value than the one that predicts every missing throughout the range of $\\lambda$. Therefore, this algorithm performs better and selectively predicting missing data is more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ending Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We implemented an Effective Missing Data Prediction (EMDP) algorithm for Collaborative Filtering as per the solution framework provided in the given paper. \n",
    "\n",
    "- This algorithm performs on combined information from and user and item similarities.\n",
    "\n",
    "- It selectively predicts missing data. \n",
    "\n",
    "- The algorithm outperformed other missing data prediction approaches, so it is effective for use in Collaborative Filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\" href=#ref1>[1]</a> Harper, F.M. and Konstan, J.A. (2015). The MovieLens Datasets: History and Context. _ACM Transactions on Interactive Intelligent Systems (TiiS)_, [online] 5(4), pp.1â€“19, Article No.: 19. doi:10.1145/2827872.\n",
    "\n",
    "<a id=\"2\" href=#ref2>[2]</a> Koren, Y., Bell, R. and Volinsky, C. (2009). Matrix Factorization Techniques for Recommender Systems. _Computer_, [online] 42(8), pp.30â€“37. doi:10.1109/mc.2009.263.\n",
    "\n",
    "<a id=\"3\" href=#ref3>[3]</a> Ma, H., King, I. and Lyu, M.R. (2007). Effective missing data prediction for collaborative filtering. In: _Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR â€™07_. [online] Amsterdam, The Netherlands: Association for Computing Machinery, pp.39â€“46. doi:10.1145/1277741.1277751.\n",
    "\n",
    "<a id=\"4\" href=#ref4>[4]</a> McDonald, C. and Moreira, G. (2021). _How to Build a Winning Recommendation System, Part 1_. [online] NVIDIA Developer Blog. Available at: https://developer.nvidia.com/blog/how-to-build-a-winning-recommendation-system-part-1/ [Accessed 11 Jun. 2021].\n",
    "\n",
    "<a id=\"5\" href=#ref5>[5]</a> Nixon, Ã€.E. (2021). _Building a Memory Based Collaborative Filtering Recommender_. [online] Towards Data Science - Medium. Available at: https://towardsdatascience.com/how-does-collaborative-filtering-work-da56ea94e331 [Accessed 11 Jun. 2019].\n",
    "\n",
    "<a id=\"6\" href=#ref6>[6]</a> Parks, J., Ramm, M. and Aurisset, J. (2020). _Innovating Faster on Personalization Algorithms at Netflix Using Interleaving_. [online] Netflix Technology Blog - Medium. Available at: https://netflixtechblog.com/interleaving-in-online-experiments-at-netflix-a04ee392ec55 [Accessed 13 Jun. 2021].\n",
    "\n",
    "<a id=\"7\" href=#ref7>[7]</a> Parveez, S. and Iriondo, R. (2021). _Recommendation System Tutorial with Python Using Collaborative Filtering_. [online] Towards AI - Medium. Available at: https://pub.towardsai.net/recommendation-system-in-depth-tutorial-with-python-for-netflix-using-collaborative-filtering-533ff8a0e444 [Accessed 10 Jun. 2021].\n",
    "\n",
    "<a id=\"8\" href=#ref8>[8]</a> Sarwar, B., Karypis, G., Konstan, J. and Reidl, J. (2001). Item-Based Collaborative Filtering Recommendation Algorithms. _Proceedings of the Tenth International Conference on World Wide Web - WWW â€™01_, [online] pp.285â€“295. doi:10.1145/371920.372071."
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "11936603/F93RFSHT": {
     "URL": "https://netflixtechblog.com/interleaving-in-online-experiments-at-netflix-a04ee392ec55",
     "abstract": "By Joshua Parks, Juliette Aurisset, Michael Ramm",
     "accessed": {
      "date-parts": [
       [
        2022,
        6,
        24
       ]
      ]
     },
     "author": [
      {
       "family": "Netflix Technology Blog",
       "given": ""
      }
     ],
     "container-title": "Medium",
     "issued": {
      "date-parts": [
       [
        2020,
        5,
        8
       ]
      ]
     },
     "language": "en",
     "title": "Innovating Faster on Personalization Algorithms at Netflix Using Interleaving",
     "type": "webpage"
    },
    "undefined": {
     "URL": "https://netflixtechblog.com/interleaving-in-online-experiments-at-netflix-a04ee392ec55",
     "abstract": "By Joshua Parks, Juliette Aurisset, Michael Ramm",
     "accessed": {
      "date-parts": [
       [
        2022,
        6,
        24
       ]
      ]
     },
     "author": [
      {
       "family": "Netflix Technology Blog",
       "given": ""
      }
     ],
     "container-title": "Medium",
     "issued": {
      "date-parts": [
       [
        2020,
        5,
        8
       ]
      ]
     },
     "language": "en",
     "title": "Innovating Faster on Personalization Algorithms at Netflix Using Interleaving",
     "type": "webpage"
    }
   }
  },
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3_framework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
